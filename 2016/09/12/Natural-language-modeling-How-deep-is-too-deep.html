<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Natural language modeling: How deep is too deep?</title>
    <meta name="description" content="" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="https://ilyaeck.github.io/favicon.ico">

    <link rel="stylesheet" type="text/css" href="//ilyaeck.github.io/themes/casper/assets/css/screen.css?v=1473878050747" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />

    <link rel="canonical" href="https://ilyaeck.github.io/2016/09/12/Natural-language-modeling-How-deep-is-too-deep.html" />
    <meta name="referrer" content="origin" />
    
    <meta property="og:site_name" content="IE " />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Natural language modeling: How deep is too deep?" />
    <meta property="og:description" content="[.lead] Is Deep Learning always the best tool for Natural Language Understanding tasks? Not necessarily! Recently, a new paper from Google research appeared on arXiv [1], immediately catching the attention of the NLP community. Focusing on the problem of Natural Language Inference (NLI), the paper made a convincing case for" />
    <meta property="og:url" content="https://ilyaeck.github.io/2016/09/12/Natural-language-modeling-How-deep-is-too-deep.html" />
    <meta property="article:tag" content="Deep Learning" />
    <meta property="article:tag" content=" NLP" />
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Natural language modeling: How deep is too deep?" />
    <meta name="twitter:description" content="[.lead] Is Deep Learning always the best tool for Natural Language Understanding tasks? Not necessarily! Recently, a new paper from Google research appeared on arXiv [1], immediately catching the attention of the NLP community. Focusing on the problem of Natural Language Inference (NLI), the paper made a convincing case for" />
    <meta name="twitter:url" content="https://ilyaeck.github.io/2016/09/12/Natural-language-modeling-How-deep-is-too-deep.html" />
    
    <script type="application/ld+json">
null
    </script>

    <meta name="generator" content="HubPress" />
    <link rel="alternate" type="application/rss+xml" title="IE " href="https://ilyaeck.github.io/rss/" />
</head>
<body class="post-template tag-Deep-Learning tag-NLP nav-closed">

    

    <div class="site-wrapper">

        


<header class="main-header post-head no-cover">
    <nav class="main-nav  clearfix">
        
    </nav>
</header>

<main class="content" role="main">
    <article class="post tag-Deep-Learning tag-NLP">

        <header class="post-header">
            <h1 class="post-title">Natural language modeling: How deep is too deep?</h1>
            <section class="post-meta">
                <time class="post-date" datetime="2016-09-12">12 September 2016</time>  on <a href="https://ilyaeck.github.io/tag/Deep-Learning/">Deep Learning</a>, <a href="https://ilyaeck.github.io/tag/NLP/"> NLP</a>
            </section>
        </header>

        <section class="post-content">
            <div class="paragraph">
<p>[.lead] Is Deep Learning always the best tool for Natural Language Understanding tasks? Not necessarily!</p>
</div>
<div class="paragraph">
<p>Recently, a new paper from Google research appeared on arXiv [1], immediately catching the attention of the NLP community. Focusing
on the problem of Natural Language Inference (NLI), the paper made a convincing case for the overall superiority of &#8230;&#8203; <strong>shallow</strong>
sentence representations - as opposed to deep ones - for NLU tasks such as NLI. Furthermore, this paper is but one example in
the recent slew of results casting the efficacy of Deep NN architectures into question for text-related tasks. Not the least of which was
FAIR&#8217;s FastText[2] (essentially a simple modification of the unsupervised Word2Vec algorithm to deal with supervised learning tasks).
See also  here[3] and here[4].</p>
</div>
<div class="paragraph">
<p>Not familiar with NLI and why it is an important litmus test? Fear not: this paragraph is about to fill you in!
(For reasers familiar with the terminology, feel free to skip to the next paragraph).
&#8592;-----&#8594;NLI EXPLANATION HERE &#8592;-------&#8594;</p>
</div>
<div class="paragraph">
<p>Hold on, you might say, isn&#8217;t Deep Learning a new disruptive force in AI, shown beyond doubt to be clearly superior to prior "shallow"
learning approaches? Well, it depends who you ask. Ask a Computer Vision or a Speech Recognition guy, and you&#8217;ll get and enthusiastic Yes!
In (computer) vision, novel DL archirectures (such as VGG, GoogleNet, Inception, etc.) have delivered extremely impressive
results on ImageNet, CIFAR, even defying expectations of some DL champions in vision [5]. In ASR, commercial heaviweights such as
Google and Baidu have longs since switch to DL architectures. It should only be natural to expect, then, to see the
same trend in NLP/NLU, correct?
Well, not so fast.</p>
</div>
<div class="paragraph">
<p>The tide of enthusiasm in Deep Learning has of course spilled over to the NLU community, triggering a massive conversion of both
academics and industry practitioners to the newfound DL religion. Impressive results from other fields,
helped by the success of the seminal Word2Vec[6] (followed by Glove[7] and the like) was too much to resist.
RNN and LSTM have since become mainstream techniques, offered by
popular DL libraries such as TensorFlow, Keras, DL4J, etc. Among other big companies, Google has been at the forefront of both
migrating to DL for text machine learning tasks (such as translation[8], SmartReply[9], etc.)</p>
</div>
<div class="paragraph">
<p>So, you ask? Isn&#8217;t that enough? Well, dear friend: if you are reading this, chances are, you are not Google! And as a result, you
don&#8217;t have the same massive amounts of training data, nor their virtually unlimited computational resources. To the rest of us,
it is important to understand the performance/computation tradeoffs that come with DL  -  which is what this post is really about.</p>
</div>
<div class="paragraph">
<p>So let us look at some key problems one by one.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Subsection: Тext Classification <strong>*</strong>
Common task with plenty of applications: search, intent classification, sentiment analysis, topic modeling
(slightly different, but close), seqeuence labeling (related), etc.
For text classification, we typically have 3 options:
A) Logit or RF with nag of words or averaged word vectors.
B) Shallow neural networks such as FastText
C) Deep networks such as RNNs/LSTMs or CNNs.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Recurrent Neural Networks (RNNs) are category of NN-based models designed specifically for sequences of arbitrary length.
Which makes particularly attractive tool for modeling text.
(2 sentences on RNN + picture: vector representation, etc.). However, in practice, RNNs can be hard to train and for small to medium-sized training datasest
, good old classifiers such as Logistic Regression or Random Forest (using a bag of words representation) can often deliver similar or even superior
performance at a lower computational cost. Even in the Deep Learning category, RNNs have a strong competitor in ConvNets
(a.k.a. Convolutional Neural Nets or CNNs) - just as long as your text can be treated as fixed length sequences, making them a suitable approach
to represent and classify tweets, text messages, short user reviews, etc. Still, it&#8217;s too early to dismiss RNNs and their variants entirely.
Here is an important distinction to realize. Many traditional classification methods aim to learn a feature space partitioning function
(as a means to separate samples of different classes), leaving feature engineering to the application developer. Conversely, neural networks
actually do more than that: they seek to learn the optimal representation (read: N-dimensional vector encoding of your data points:
pixels, words, sentences, pages, what have you&#8230;&#8203;) so as to minimize the (problem-dependent) loss function on the training set. That learned
representation is a byproduct that can sometimes be more valuable than the main task. For instance, in Word2Vec, the task is predicting a word
based on the words around it (or vice-versa), which admittedly is not a very common problem in reality. However, as a byproduct, we get word vectors
with very interesting semantic properties (link) that become handy in text classification and other application.</p>
</div>
<div class="paragraph">
<p>Subsection: NLI and machine translation.
Why is NLI important ? It&#8217;s a key problem in natural language understanding. Many other NLU problems can be reduced to NLI: such as summarization
(given a piece of text and a suggested summary, does the former entail the latter), information extraction (does the text entail the extracted fact),
question answering (does the data source entail a given question and answer pair) as well as machine machine translation
(does a phrase in language A entail its given translation in language B and vice versa).
Why is it mentioned in the same category as machine translation? Both can be cast as an alignment problem.</p>
</div>
<div class="paragraph">
<p>Subsection: Question Answering</p>
</div>
<div class="paragraph">
<p>Subsection: Reading comprehension, memory and attention.</p>
</div>
<div class="paragraph">
<p>Subsection: Dialogue and chatbots!</p>
</div>
<div class="paragraph">
<p>So why hs DL been more successful in ASR and vision than NLP/NLU? That&#8217;s a topic for another post!</p>
</div>
<div class="paragraph">
<p>[5] Karpathy
[6] Word2Vec
[7] Glove
[8] Google Translate
[9] Smart Reply
[10] <a href="http://www.foldl.me/2016/solving-language/" class="bare">http://www.foldl.me/2016/solving-language/</a></p>
</div>
        </section>

        <footer class="post-footer">


            <figure class="author-image">
                <a class="img" href="https://ilyaeck.github.io/author/ilyaeck/" style="background-image: url(https://avatars.githubusercontent.com/u/156822?v&#x3D;3)"><span class="hidden">Ilya Eckstein 's Picture</span></a>
            </figure>

            <section class="author">
                <h4><a href="https://ilyaeck.github.io/author/ilyaeck/">Ilya Eckstein </a></h4>

                    <p>Read <a href="https://ilyaeck.github.io/author/ilyaeck/">more posts</a> by this author.</p>
                <div class="author-meta">
                    
                    
                </div>
            </section>


            <section class="share">
                <h4>Share this post</h4>
                <a class="icon-twitter" href="https://twitter.com/intent/tweet?text=Natural%20language%20modeling%3A%20How%20deep%20is%20too%C2%A0deep%3F&amp;url=https://ilyaeck.github.io/2016/09/12/Natural-language-modeling-How-deep-is-too-deep.html"
                    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://ilyaeck.github.io/2016/09/12/Natural-language-modeling-How-deep-is-too-deep.html"
                    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=https://ilyaeck.github.io/2016/09/12/Natural-language-modeling-How-deep-is-too-deep.html"
                   onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>

        </footer>


    </article>

</main>

<aside class="read-next">
</aside>



        <footer class="site-footer clearfix">
            <section class="copyright"><a href="https://ilyaeck.github.io">IE </a> &copy; 2016</section>
            <section class="poweredby">Proudly published with <a href="http://hubpress.io">HubPress</a></section>
        </footer>

    </div>

    <script type="text/javascript" src="https://code.jquery.com/jquery-1.12.0.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.9.0/moment-with-locales.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js?v="></script> 
      <script type="text/javascript">
        jQuery( document ).ready(function() {
          // change date with ago
          jQuery('ago.ago').each(function(){
            var element = jQuery(this).parent();
            element.html( moment(element.text()).fromNow());
          });
        });

        hljs.initHighlightingOnLoad();
      </script>

    <script type="text/javascript" src="//ilyaeck.github.io/themes/casper/assets/js/jquery.fitvids.js?v=1473878050747"></script>
    <script type="text/javascript" src="//ilyaeck.github.io/themes/casper/assets/js/index.js?v=1473878050747"></script>

</body>
</html>
