= Natural language modeling: How deep is too deep? -- Part I -- 
:hp-tags: Deep Learning, NLP
Is Deep Learning always the best tool for Natural Language Understanding tasks? Not necessarily!

Recently, a new paper from Google research appeared on arXiv [1], immediately catching the attention of the NLP community. Focusing 
on the problem of Natural Language Inference (NLI), the paper made a convincing case for the overall superiority of ... *shallow* 
sentence representations - as opposed to deep ones - for NLU tasks such as NLI. Furthermore, this paper is but one example in 
the recent slew of results casting the efficacy of Deep NN architectures into question for text-related tasks. Not the least of which was 
FAIR's FastText[2] (essentially a simple modification of the unsupervised Word2Vec algorithm to deal with supervised learning tasks). 
See also  here[3] and here[4]. 

Not familiar with NLI and why it is an important litmus test? Fear not: this paragraph is about to fill you in! 
(For reasers familiar with the terminology, feel free to skip to the next paragraph). 
<------->NLI EXPLANATION HERE <--------->

Hold on, you might say, isn't Deep Learning a new disruptive force in AI, shown beyond doubt to be clearly superior to prior "shallow"
learning approaches? Well, it depends who you ask. Ask a Computer Vision or a Speech Recognition guy, and you'll get and enthusiastic Yes!
In (computer) vision, novel DL archirectures (such as VGG, GoogleNet, Inception, etc.) have delivered extremely impressive 
results on ImageNet, CIFAR, even defying expectations of some DL champions in vision [5]. In ASR, commercial heaviweights such as 
Google and Baidu have longs since switch to DL architectures. It should only be natural to expect, then, to see the 
same trend in NLP/NLU, correct? 
Well, not so fast. 

The tide of enthusiasm in Deep Learning has of course spilled over to the NLU community, triggering a massive conversion of both 
academics and industry practitioners to the newfound DL religion. Impressive results from other fields, 
helped by the success of the seminal Word2Vec[6] (followed by Glove[7] and the like) was too much to resist. 
RNN and LSTM have since become mainstream techniques, offered by 
popular DL libraries such as TensorFlow, Keras, DL4J, etc. Among other big companies, Google has been at the forefront of both 
migrating to DL for text machine learning tasks (such as translation[8], SmartReply[9], etc.)

So, you ask? Isn't that enough? Well, dear friend: if you are reading this, chances are, you are not Google! And as a result, you 
don't have the same massive amounts of training data, nor their virtually unlimited computational resources. To the rest of us, 
it is important to understand the performance/computation tradeoffs that come with DL  -  which is what this post is really about. 

So let us look at some key problems one by one. 

*** Subsection: Тext Classification ***
Common task with plenty of applications: search, intent classification, sentiment analysis, topic modeling 
(slightly different, but close), seqeuence labeling (related), etc. 
For text classification, we typically have 3 options: 
A) Logit or RF with nag of words or averaged word vectors.  
B) Shallow neural networks such as FastText
C) Deep networks such as RNNs/LSTMs or CNNs. 

Recurrent Neural Networks (RNNs) are category of NN-based models designed specifically for sequences of arbitrary length. 
Which makes particularly attractive tool for modeling text. 
(2 sentences on RNN + picture: vector representation, etc.). However, in practice, RNNs can be hard to train and for small to medium-sized training datasest 
, good old classifiers such as Logistic Regression or Random Forest (using a bag of words representation) can often deliver similar or even superior 
performance at a lower computational cost. Even in the Deep Learning category, RNNs have a strong competitor in ConvNets 
(a.k.a. Convolutional Neural Nets or CNNs) - just as long as your text can be treated as fixed length sequences, making them a suitable approach 
to represent and classify tweets, text messages, short user reviews, etc. Still, it's too early to dismiss RNNs and their variants entirely.  
Here is an important distinction to realize. Many traditional classification methods aim to learn a feature space partitioning function
(as a means to separate samples of different classes), leaving feature engineering to the application developer. Conversely, neural networks
actually do more than that: they seek to learn the optimal representation (read: N-dimensional vector encoding of your data points: 
pixels, words, sentences, pages, what have you...) so as to minimize the (problem-dependent) loss function on the training set. That learned 
representation is a byproduct that can sometimes be more valuable than the main task. For instance, in Word2Vec, the task is predicting a word 
based on the words around it (or vice-versa), which admittedly is not a very common problem in reality. However, as a byproduct, we get word vectors 
with very interesting semantic properties (link) that become handy in text classification and other application. 



Subsection: NLI and machine translation. 
Why is NLI important ? It's a key problem in natural language understanding. Many other NLU problems can be reduced to NLI: such as summarization 
(given a piece of text and a suggested summary, does the former entail the latter), information extraction (does the text entail the extracted fact), 
question answering (does the data source entail a given question and answer pair) as well as machine machine translation 
(does a phrase in language A entail its given translation in language B and vice versa).
Why is it mentioned in the same category as machine translation? Both can be cast as an alignment problem.  


Subsection: Question Answering 



Subsection: Reading comprehension, memory and attention. 


Subsection: Dialogue and chatbots! 


So why hs DL been more successful in ASR and vision than NLP/NLU? That's a topic for another post! 



[1] http://arxiv.org/pdf/1606.01933v1.pdf [A Decomposable Attention Model for Natural Language Inference] 
[2]
[3]
[4]
[5] Karpathy
[6] Word2Vec
[7] Glove
[8] Google Translate
[9] Smart Reply
[10] http://www.foldl.me/2016/solving-language/

 

