<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[IE ]]></title><description><![CDATA[Ilya Eckstein's blog]]></description><link>https://ilyaeck.github.io</link><generator>RSS for Node</generator><lastBuildDate>Wed, 14 Sep 2016 20:40:02 GMT</lastBuildDate><atom:link href="https://ilyaeck.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Natural language modeling: How deep is too deep?]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><strong>TL;DR</strong> Is Deep Learning always the best tool for Natural Language Understanding tasks? Not necessarily!</p>
</div>
<div class="paragraph">
<p>Recently, a paper from Facebook AI Research (FAIR) appeared on arXiv, under the intriguing title <a href="https://arxiv.org/pdf/1607.01759v2.pdf"><em>Bag of Tricks for Efficient Text Classification</em></a>[1], promptly catching the NLP community&#8217;s attentuion. Even more intruguingly, FAIR soon followed up with an open source implementation a.k.a. <a href="https://github.com/facebookresearch/fastText">fastText</a>[2]. In this work (essentially a simple modification of the unsupervised <a href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a> algorithm to deal with supervised learning tasks), the authors made a convincing case for the frequent superiority of <strong>shallow</strong>
text representations - as opposed to deep ones - for common NLU tasks such as sentence classification, dispelling the myth that "deeper is always better".
Furthermore, this paper is but one example in the recent slew of results casting the "silver bullet" efficacy of complex neural architectures into question for text-related tasks (see some examples here[3], here[4] and here[5]).</p>
</div>
<div class="paragraph">
<p>Hold on, you might say, isn&#8217;t Deep Learning a new disruptive force in AI, shown beyond doubt to be clearly superior to prior "shallow"
learning approaches? Well, it depends who you ask. Ask a Computer Vision or a Speech Recognition expert, and you&#8217;ll get and enthusiastic Yes!
In computer vision, novel DL archirectures (such as VGG, GoogleNet, Inception, etc.) have delivered extremely impressive
results on ImageNet, CIFAR, even defying expectations of some computer vision DL champions[5]. In speech, commercial heavyweights such as
Google and Baidu have longs since switch to DL architectures. It should only be natural to expect, then, to see the
same trend in NLP/NLU, correct?
Well, not so fast.</p>
</div>
<div class="paragraph">
<p>The tide of enthusiasm in Deep Learning has of course spilled over to the NLU community, triggering a massive conversion of both
academics and industry practitioners to the newfound DL religion. Impressive results from other fields,
helped by the success of the seminal Word2Vec[6] (followed by Glove[7] and the like) was too much to resist. RNN and LSTM have since become mainstream techniques, offered by
popular DL libraries such as TensorFlow, Keras, DL4J, etc. Among other big companies, Google has been at the forefront of both open-sourcing DL techniques (with <a href="https://www.tensorflow.org/">TensorFlow</a>) and adopting DL architectures in production (see, e.g., SmartReply[9]).</p>
</div>
<div class="paragraph">
<p>So - you ask - isn&#8217;t that enough? Where do I sign up?! Well, dear friend: if you are reading this, chances are, you are not Google! And as a result, you
don&#8217;t have the same massive amounts of training data, nor their virtually unlimited computational resources. To the rest of us,
it is important to understand the performance/computation tradeoffs that come with DL  -  which is what this post is really about.</p>
</div>
<div class="paragraph">
<p>So let us look at some key problems one by one. In this post, we will focus on text classification.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="__ext_classification">Тext Classification</h3>
<div class="paragraph">
<p>Clearly, text classification is a common task with plenty of applications: search, user intent determination, sentiment analysis, topic modeling
(slightly different, but close), seqeuence labeling (related), etc.
For text classification, we typically have 3 options:</p>
</div>
<div class="ulist disc">
<ul class="disc">
<li>
<p>Good old classifiers such as Random Forests (RF) or Logistic Regression, using a Bag of Word (BoW) representation such as TF/IDF or averaged Continuous-BOW word vectors.</p>
</li>
<li>
<p>Shallow neural networks such as Facebook&#8217;s fastText</p>
</li>
<li>
<p>Deep networks such as RNNs/LSTMs or CNNs.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Recurrent Neural Networks (RNNs) are category of NN-based models designed specifically for sequences of arbitrary length.
Which makes particularly attractive tool for modeling text.
(2 sentences on RNN + picture: vector representation, etc.). However, in practice, RNNs can be hard to train and for small to medium-sized training datasest
, good old classifiers such as Logistic Regression or Random Forest (using a bag of words representation) can often deliver similar or even superior
performance at a lower computational cost. Even in the Deep Learning category, RNNs have a strong competitor in ConvNets
(a.k.a. Convolutional Neural Nets or CNNs) - just as long as your text can be treated as fixed length sequences, making them a suitable approach
to represent and classify tweets, text messages, short user reviews, etc. Still, it&#8217;s too early to dismiss RNNs and their variants entirely.
Here is an important distinction to realize. Many traditional classification methods aim to learn a feature space partitioning function
(as a means to separate samples of different classes), leaving feature engineering to the application developer. Conversely, neural networks
actually do more than that: they seek to learn the optimal representation (read: N-dimensional vector encoding of your data points:
pixels, words, sentences, pages, what have you&#8230;&#8203;) so as to minimize some loss function on the training set. That learned representation is a byproduct that can sometimes be more valuable than the main task! For instance, in Word2Vec, the task is predicting a word based on the words around it (or vice-versa), which admittedly is not a very common problem in reality. However, as a byproduct, we get word vectors
with very interesting semantic properties (link) that become handy in text classification and other application.</p>
</div>
</div>
<div class="sect2">
<h3 id="_nli_and_machine_translation">NLI and machine translation.</h3>
<div class="paragraph">
<p>Why is NLI important ? It&#8217;s a key problem in natural language understanding. Many other NLU problems can be reduced to NLI: such as summarization
(given a piece of text and a suggested summary, does the former entail the latter), information extraction (does the text entail the extracted fact),
question answering (does the data source entail a given question and answer pair) as well as machine machine translation
(does a phrase in language A entail its given translation in language B and vice versa).
Why is it mentioned in the same category as machine translation? Both can be cast as an alignment problem.</p>
</div>
<div class="paragraph">
<p>Subsection: Question Answering</p>
</div>
<div class="paragraph">
<p>Subsection: Reading comprehension, memory and attention.</p>
</div>
<div class="paragraph">
<p>Subsection: Dialogue and chatbots!</p>
</div>
<div class="paragraph">
<p>So why hs DL been more successful in ASR and vision than NLP/NLU? That&#8217;s a topic for another post!</p>
</div>
<div class="sect3">
<h4 id="_references">References</h4>
<div class="paragraph">
<p>[1] <a href="https://arxiv.org/pdf/1607.01759v2.pdf">Bag of Tricks for Efficient Text Classification</a>, A. Joulin, E. Grave, P. Bojanowski, T. Mikolov</p>
</div>
<div class="paragraph">
<p>[3] <a href="http://arxiv.org/abs/1608.04207v1">Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks</a>, Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, Yoav Goldberg
[4] <a href="http://arxiv.org/pdf/1606.02858v2.pdf">A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</a>, Danqi Chen, Jason Bolton, Christopher D. Manning
[5] <a href="http://arxiv.org/pdf/1606.01933v1.pdf">A Decomposable Attention Model for Natural Language Inference</a>, Ankur P. Parikh, Oscar Täckström, Dipanjan Das, Jakob Uszkoreit
[6] Karpathy
[7] Word2Vec
[8] Glove
[9] Smart Reply
[10] <a href="http://www.foldl.me/2016/solving-language/" class="bare">http://www.foldl.me/2016/solving-language/</a></p>
</div>
</div>
</div>]]></description><link>https://ilyaeck.github.io/2016/09/12/Natural-language-modeling-How-deep-is-too-deep.html</link><guid isPermaLink="true">https://ilyaeck.github.io/2016/09/12/Natural-language-modeling-How-deep-is-too-deep.html</guid><category><![CDATA[Deep Learning]]></category><category><![CDATA[ NLP]]></category><dc:creator><![CDATA[Ilya Eckstein ]]></dc:creator><pubDate>Mon, 12 Sep 2016 00:00:00 GMT</pubDate></item></channel></rss>